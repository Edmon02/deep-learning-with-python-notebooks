{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ],
      "metadata": {
        "id": "Qkr1_BWJz_QE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beyond text classification: Sequence-to-sequence learning"
      ],
      "metadata": {
        "id": "Oy-EKxzv0BtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A machine translation example"
      ],
      "metadata": {
        "id": "hz-RhY8W0Cjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
        "!unzip -q spa-eng.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0egxOepy0DUi",
        "outputId": "ccd488db-4a97-4a41-ae5e-f73e3e6abb56"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-09 13:40:53--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.153.128, 142.250.145.128, 74.125.128.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.153.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2638744 (2.5M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "spa-eng.zip         100%[===================>]   2.52M  5.59MB/s    in 0.5s    \n",
            "\n",
            "2023-07-09 13:40:54 (5.59 MB/s) - ‘spa-eng.zip’ saved [2638744/2638744]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = \"spa-eng/spa.txt\"\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    english, spanish = line.split(\"\\t\")\n",
        "    spanish = \"[start] \" + spanish + \" [end]\"\n",
        "    text_pairs.append((english, spanish))"
      ],
      "metadata": {
        "id": "gyH7KPce1KGx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "print(random.choice(text_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWogPAzv1O-A",
        "outputId": "bbc257fc-5b57-4985-e159-79b89ed723ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('I want to eat apple pie.', '[start] Quiero comer tarta de manzana. [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ],
      "metadata": {
        "id": "8KfXn0vq1fuE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vectorizing the English and Spanish text pairs**"
      ],
      "metadata": {
        "id": "gZlif5oo1kA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)"
      ],
      "metadata": {
        "id": "fP8OgsHG2CPY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparing datasets for the translation task**"
      ],
      "metadata": {
        "id": "kpMg3r0L2ViH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, spa):\n",
        "    eng = source_vectorization(eng)\n",
        "    spa = target_vectorization(spa)\n",
        "    return ({\n",
        "        \"english\": eng,\n",
        "        \"spanish\": spa[:, :-1],\n",
        "    }, spa[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "yyvzLoHF22ev"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jfyfVe4255G",
        "outputId": "e00f459a-b0ac-4b58-fc5b-8ebf973be560"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['spanish'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequence-to-sequence learning with RNNs"
      ],
      "metadata": {
        "id": "UBtrv9P33T6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRU-based encoder**"
      ],
      "metadata": {
        "id": "zJWV-hwz4ijo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "embed_dim = 256\n",
        "latent_dim = 1024\n",
        "\n",
        "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
        "encoded_source = layers.Bidirectional(\n",
        "    layers.GRU(latent_dim), merge_mode=\"sum\")(x)"
      ],
      "metadata": {
        "id": "PAdyt3O24jbP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRU-based decoder and the end-to-end model**"
      ],
      "metadata": {
        "id": "2CiKDq414u8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
        "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
        "x = decoder_gru(x, initial_state=encoded_source)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
      ],
      "metadata": {
        "id": "ftchcgPN45IW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training our recurrent sequence-to-sequence model**"
      ],
      "metadata": {
        "id": "2xD_G4rS456P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_rnn.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9P0YMclC5fPg",
        "outputId": "f1da6f18-233a-4f8b-e951-a981693dae4b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1302/1302 [==============================] - 162s 109ms/step - loss: 4.6893 - accuracy: 0.3182 - val_loss: 3.9282 - val_accuracy: 0.3839\n",
            "Epoch 2/15\n",
            "1302/1302 [==============================] - 111s 85ms/step - loss: 3.7322 - accuracy: 0.4138 - val_loss: 3.2508 - val_accuracy: 0.4678\n",
            "Epoch 3/15\n",
            "1302/1302 [==============================] - 112s 86ms/step - loss: 3.2190 - accuracy: 0.4719 - val_loss: 2.8684 - val_accuracy: 0.5180\n",
            "Epoch 4/15\n",
            "1302/1302 [==============================] - 123s 94ms/step - loss: 2.8631 - accuracy: 0.5132 - val_loss: 2.6095 - val_accuracy: 0.5547\n",
            "Epoch 5/15\n",
            "1302/1302 [==============================] - 111s 86ms/step - loss: 2.5842 - accuracy: 0.5467 - val_loss: 2.4303 - val_accuracy: 0.5793\n",
            "Epoch 6/15\n",
            "1302/1302 [==============================] - 113s 86ms/step - loss: 2.3585 - accuracy: 0.5750 - val_loss: 2.3028 - val_accuracy: 0.5995\n",
            "Epoch 7/15\n",
            "1302/1302 [==============================] - 113s 87ms/step - loss: 2.1770 - accuracy: 0.5986 - val_loss: 2.2084 - val_accuracy: 0.6140\n",
            "Epoch 8/15\n",
            "1302/1302 [==============================] - 112s 86ms/step - loss: 2.0164 - accuracy: 0.6202 - val_loss: 2.1423 - val_accuracy: 0.6254\n",
            "Epoch 9/15\n",
            "1302/1302 [==============================] - 112s 86ms/step - loss: 1.8752 - accuracy: 0.6393 - val_loss: 2.0985 - val_accuracy: 0.6325\n",
            "Epoch 10/15\n",
            "1302/1302 [==============================] - 112s 86ms/step - loss: 1.7571 - accuracy: 0.6556 - val_loss: 2.0367 - val_accuracy: 0.6414\n",
            "Epoch 11/15\n",
            "1302/1302 [==============================] - 112s 86ms/step - loss: 1.6530 - accuracy: 0.6698 - val_loss: 2.0084 - val_accuracy: 0.6461\n",
            "Epoch 12/15\n",
            "1302/1302 [==============================] - 112s 86ms/step - loss: 1.5646 - accuracy: 0.6825 - val_loss: 1.9808 - val_accuracy: 0.6515\n",
            "Epoch 13/15\n",
            "1302/1302 [==============================] - 113s 87ms/step - loss: 1.4887 - accuracy: 0.6937 - val_loss: 1.9575 - val_accuracy: 0.6554\n",
            "Epoch 14/15\n",
            "1302/1302 [==============================] - 112s 86ms/step - loss: 1.4107 - accuracy: 0.7044 - val_loss: 1.9429 - val_accuracy: 0.6580\n",
            "Epoch 15/15\n",
            "1302/1302 [==============================] - 113s 87ms/step - loss: 1.3502 - accuracy: 0.7140 - val_loss: 1.9305 - val_accuracy: 0.6607\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe9d8d8f400>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Translating new sentences with our RNN encoder and decoder**"
      ],
      "metadata": {
        "id": "c2CTsdKy5f9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
        "        next_token_predictions = seq2seq_rnn.predict(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIISH5cl5-Jl",
        "outputId": "a057d953-7b27-48e3-d0ca-901934e913dc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "Tom is married to a Canadian.\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "[start] tom está casado con un canadiense [end]\n",
            "-\n",
            "Do what you have to do.\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "[start] haz lo que tienes que hacer [end]\n",
            "-\n",
            "Did you have fun last night?\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "[start] lo has anoche anoche [end]\n",
            "-\n",
            "The boy tried moving the heavy sofa.\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "[start] el niño se fue [UNK] el techo [end]\n",
            "-\n",
            "Please shine those shoes.\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[start] por favor [UNK] esos zapatos [end]\n",
            "-\n",
            "Are you involved in this?\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "[start] estás en esto [end]\n",
            "-\n",
            "Tom was the first one to recognize Mary's musical talent.\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "[start] tom fue el primer [UNK] para [UNK] el [UNK] de mary [end]\n",
            "-\n",
            "She had no dress to attend the party in.\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[start] ella no tenía una hermana para ver a la fiesta [end]\n",
            "-\n",
            "I took my shoes off.\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "[start] me quité los zapatos [end]\n",
            "-\n",
            "Tom is a very sensible person.\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "[start] tom es una persona muy amable [end]\n",
            "-\n",
            "We all work together.\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "[start] todos juntos juntos [end]\n",
            "-\n",
            "Who's going?\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "[start] quién va a ir [end]\n",
            "-\n",
            "He lives in a world of fantasy.\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "[start] Él vive en una situación de la ciudad [end]\n",
            "-\n",
            "You never say that you love me.\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "[start] nunca me dijiste que me amas [end]\n",
            "-\n",
            "You have to keep your promise.\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "[start] tienes que [UNK] tu promesa [end]\n",
            "-\n",
            "He was a very smart lawyer and politician.\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "[start] Él era un hombre muy [UNK] y [UNK] [end]\n",
            "-\n",
            "Tom needed the money.\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "[start] tom necesita el dinero [end]\n",
            "-\n",
            "This work's driving me crazy.\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "[start] este trabajo me estoy loco [end]\n",
            "-\n",
            "I thought it'd get easier.\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "[start] pensé que sería más fácil [end]\n",
            "-\n",
            "I thought Tom was with Mary.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "[start] pensé que tom estaba con mary [end]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequence-to-sequence learning with Transformer"
      ],
      "metadata": {
        "id": "F58vTvv36Wl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The Transformer decoder"
      ],
      "metadata": {
        "id": "BXZ7cu4GBcNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The `TransformerDecoder`**"
      ],
      "metadata": {
        "id": "L3pX7bQrBdRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "            padding_mask = mask\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ],
      "metadata": {
        "id": "hM2JSt50BeWf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Putting it all together: A Transformer for machine translation"
      ],
      "metadata": {
        "id": "LeKNZBFSFCyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PositionalEmbedding layer**"
      ],
      "metadata": {
        "id": "xsVvMrSgFDkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "eH9AdLdUFEjm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**End-to-end Transformer**"
      ],
      "metadata": {
        "id": "XU1hxtGTFJNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "XhP86frEFOeM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the sequence-to-sequence Transformer**"
      ],
      "metadata": {
        "id": "GGi-Ms99FP1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU2u0aXBF5Wo",
        "outputId": "e4b71516-e3cb-4cc8-ffe9-17c9f813d7a1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1302/1302 [==============================] - 113s 80ms/step - loss: 3.7924 - accuracy: 0.4406 - val_loss: 2.8872 - val_accuracy: 0.5390\n",
            "Epoch 2/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 2.8509 - accuracy: 0.5499 - val_loss: 2.5306 - val_accuracy: 0.5913\n",
            "Epoch 3/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 2.5569 - accuracy: 0.5931 - val_loss: 2.4220 - val_accuracy: 0.6081\n",
            "Epoch 4/30\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 2.3940 - accuracy: 0.6199 - val_loss: 2.3537 - val_accuracy: 0.6193\n",
            "Epoch 5/30\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 2.2874 - accuracy: 0.6383 - val_loss: 2.3025 - val_accuracy: 0.6318\n",
            "Epoch 6/30\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 2.2183 - accuracy: 0.6511 - val_loss: 2.2976 - val_accuracy: 0.6377\n",
            "Epoch 7/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 2.1579 - accuracy: 0.6626 - val_loss: 2.3190 - val_accuracy: 0.6402\n",
            "Epoch 8/30\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 2.0982 - accuracy: 0.6744 - val_loss: 2.2643 - val_accuracy: 0.6484\n",
            "Epoch 9/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 2.0418 - accuracy: 0.6858 - val_loss: 2.2525 - val_accuracy: 0.6557\n",
            "Epoch 10/30\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 1.9941 - accuracy: 0.6941 - val_loss: 2.2169 - val_accuracy: 0.6593\n",
            "Epoch 11/30\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 1.9534 - accuracy: 0.7013 - val_loss: 2.2337 - val_accuracy: 0.6607\n",
            "Epoch 12/30\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 1.9211 - accuracy: 0.7070 - val_loss: 2.2402 - val_accuracy: 0.6644\n",
            "Epoch 13/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.8923 - accuracy: 0.7126 - val_loss: 2.2534 - val_accuracy: 0.6600\n",
            "Epoch 14/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.8659 - accuracy: 0.7166 - val_loss: 2.2664 - val_accuracy: 0.6655\n",
            "Epoch 15/30\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 1.8472 - accuracy: 0.7211 - val_loss: 2.2483 - val_accuracy: 0.6653\n",
            "Epoch 16/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.8253 - accuracy: 0.7250 - val_loss: 2.2733 - val_accuracy: 0.6664\n",
            "Epoch 17/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.8094 - accuracy: 0.7280 - val_loss: 2.2965 - val_accuracy: 0.6688\n",
            "Epoch 18/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.7917 - accuracy: 0.7306 - val_loss: 2.2876 - val_accuracy: 0.6641\n",
            "Epoch 19/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.7744 - accuracy: 0.7339 - val_loss: 2.3124 - val_accuracy: 0.6641\n",
            "Epoch 20/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.7602 - accuracy: 0.7370 - val_loss: 2.3340 - val_accuracy: 0.6664\n",
            "Epoch 21/30\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 1.7451 - accuracy: 0.7398 - val_loss: 2.3404 - val_accuracy: 0.6683\n",
            "Epoch 22/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.7266 - accuracy: 0.7430 - val_loss: 2.3426 - val_accuracy: 0.6693\n",
            "Epoch 23/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.7153 - accuracy: 0.7446 - val_loss: 2.3694 - val_accuracy: 0.6658\n",
            "Epoch 24/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.7032 - accuracy: 0.7467 - val_loss: 2.3799 - val_accuracy: 0.6695\n",
            "Epoch 25/30\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 1.6848 - accuracy: 0.7502 - val_loss: 2.4102 - val_accuracy: 0.6626\n",
            "Epoch 26/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.6720 - accuracy: 0.7528 - val_loss: 2.3921 - val_accuracy: 0.6689\n",
            "Epoch 27/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.6608 - accuracy: 0.7546 - val_loss: 2.3900 - val_accuracy: 0.6694\n",
            "Epoch 28/30\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 1.6486 - accuracy: 0.7565 - val_loss: 2.4015 - val_accuracy: 0.6711\n",
            "Epoch 29/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.6363 - accuracy: 0.7587 - val_loss: 2.4370 - val_accuracy: 0.6697\n",
            "Epoch 30/30\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 1.6250 - accuracy: 0.7603 - val_loss: 2.4439 - val_accuracy: 0.6703\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe9d0311240>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Translating new sentences with our Transformer model**"
      ],
      "metadata": {
        "id": "Sq5qdM0xF59f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization(\n",
        "            [decoded_sentence])[:, :-1]\n",
        "        predictions = transformer(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d0Tz3rPGAzh",
        "outputId": "2ed3afde-8553-469f-e661-558b53935225"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "She was asked to help him paint the house.\n",
            "[start] le pidió que le pidió que fuera a [UNK] la casa [end]\n",
            "-\n",
            "Are you going to come visit me?\n",
            "[start] vas a visitar [end]\n",
            "-\n",
            "I'm getting married to her in June.\n",
            "[start] me voy a [UNK] en junio [end]\n",
            "-\n",
            "It is too hot.\n",
            "[start] es demasiado calor [end]\n",
            "-\n",
            "Tom winked.\n",
            "[start] tom le hizo un reloj [end]\n",
            "-\n",
            "I love summer rain.\n",
            "[start] yo encanta el verano lluvia [end]\n",
            "-\n",
            "He will soon be able to swim well.\n",
            "[start] Él pronto [UNK] [UNK] nadar bien [end]\n",
            "-\n",
            "The frescoes of the cathedral are very interesting.\n",
            "[start] la [UNK] de los [UNK] es muy interesante [end]\n",
            "-\n",
            "I'm going to take the 10:30 train.\n",
            "[start] voy a tomar el tren de vuelta de la hora [end]\n",
            "-\n",
            "Tom's shoes are too big for him.\n",
            "[start] los zapatos de tomás son demasiado grande para él [end]\n",
            "-\n",
            "She confronted him and demanded an apology.\n",
            "[start] ella se le ocurrió una diferencia y una persona de [UNK] [end]\n",
            "-\n",
            "I was nearly hit by a car.\n",
            "[start] casi lo quedé en auto [end]\n",
            "-\n",
            "They're making too much noise. I can't concentrate.\n",
            "[start] están haciendo mucho ruido no puedo [UNK] [end]\n",
            "-\n",
            "He buys only what'll be useful for him.\n",
            "[start] Él no solo compró para ser útil [end]\n",
            "-\n",
            "I'd like to study German, but I don't have the time.\n",
            "[start] me gustaría estudiar alemán pero no tengo tiempo [end]\n",
            "-\n",
            "Children like gummy bears.\n",
            "[start] a los niños les gustan los doce [UNK] [end]\n",
            "-\n",
            "It takes two to tango.\n",
            "[start] toma dos viejos para [UNK] [end]\n",
            "-\n",
            "Shut your big mouth.\n",
            "[start] cuidado con la boca grande [end]\n",
            "-\n",
            "Tom must love Mary very much.\n",
            "[start] tom debe de amor mucho a mary [end]\n",
            "-\n",
            "The dining car's at the front end of the train.\n",
            "[start] los autos a la [UNK] del final del tren [end]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary"
      ],
      "metadata": {
        "id": "kKige98NGDA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* There are two kinds of NLP models: bag-of-words models that process sets of words\n",
        "or N-grams without taking into account their order, and sequence models that process word order. A bag-of-words model is made of Dense layers, while a sequence\n",
        "model could be an RNN, a 1D convnet, or a Transformer.\n",
        "* When it comes to text classification, the ratio between the number of samples\n",
        "in your training data and the mean number of words per sample can help you\n",
        "determine whether you should use a bag-of-words model or a sequence model.\n",
        "* Word embeddings are vector spaces where semantic relationships between words are\n",
        "modeled as distance relationships between vectors that represent those words.\n",
        "* Sequence-to-sequence learning is a generic, powerful learning framework that can be\n",
        "applied to solve many NLP problems, including machine translation. A sequenceto-sequence model is made of an encoder, which processes a source sequence,\n",
        "and a decoder, which tries to predict future tokens in target sequence by looking\n",
        "at past tokens, with the help of the encoder-processed source sequence.\n",
        "* Neural attention is a way to create context-aware word representations. It’s the\n",
        "basis for the Transformer architecture.\n",
        "* The Transformer architecture, which consists of a TransformerEncoder and a\n",
        "TransformerDecoder, yields excellent results on sequence-to-sequence tasks.\n",
        "The first half, the TransformerEncoder, can also be used for text classification\n",
        "or any sort of single-input NLP task."
      ],
      "metadata": {
        "id": "SKrNTNn5G2OP"
      }
    }
  ]
}